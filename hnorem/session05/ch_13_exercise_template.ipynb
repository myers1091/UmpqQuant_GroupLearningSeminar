{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI0/zDJXZ73FXY8gfZlA0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myers1091/UmpqQuant_GroupLearningSeminar/blob/main/hnorem/session05/ch_13_exercise_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hb7H9tTBDvKz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.train import Feature, Features, Example, BytesList, FloatList, Int64List\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "\n",
        "from tensorflow.keras import utils\n",
        "import collections\n",
        "import pathlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise! \n",
        "\n",
        "Let's repeat what we did for exercise 10 of chapter 13, but on the dataset below. What needs to be adapted and how? I have no idea :D\n",
        "\n",
        "I have included my solution below this."
      ],
      "metadata": {
        "id": "kVpwMPEtepAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "\n",
        "dataset_dir = utils.get_file(\n",
        "    origin=data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')\n",
        "\n",
        "dataset_dir = pathlib.Path(dataset_dir).parent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Ghnnwvenxn",
        "outputId": "0e390f04-3395-4dc1-d9a5-2f511fe3e1ed"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053168/6053168 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(dataset_dir.iterdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH2bX0tkfF66",
        "outputId": "1649e8ca-bd1f-4502-9887-23d9e17a8f15"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/test'),\n",
              " PosixPath('/tmp/.keras/stack_overflow_16k.tar.gz'),\n",
              " PosixPath('/tmp/.keras/README.md'),\n",
              " PosixPath('/tmp/.keras/train')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = dataset_dir/'train'\n",
        "list(train_dir.iterdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmK0YrHhfKtT",
        "outputId": "3d96d99f-51cb-421c-e8fc-64599543f143"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/tmp/.keras/train/java'),\n",
              " PosixPath('/tmp/.keras/train/python'),\n",
              " PosixPath('/tmp/.keras/train/csharp'),\n",
              " PosixPath('/tmp/.keras/train/javascript')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_file = train_dir/'python/1755.txt'\n",
        "\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDG8so3yfQdM",
        "outputId": "30a8d862-bd34-4bae-cd94-df8f1779c1f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cheat sheet available here but let's try to look as little as possible:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/text"
      ],
      "metadata": {
        "id": "to4msI-JfVjF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I45L8tj0fU4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 10\n",
        "\n",
        "Exercise: In this exercise you will download a dataset, split it, create a tf.data.Dataset to load it and preprocess it efficiently, then build and train a binary classification model containing an Embedding layer."
      ],
      "metadata": {
        "id": "nYlPQqx9blQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PART A - C. \n",
        "\n",
        "Download the Large Movie Review Dataset, which contains 50,000 movies reviews from the Internet Movie Database. The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise.\n",
        "\n",
        "Split validation into test and validation.\n",
        "\n",
        "Use tf.data to create an efficient dataset for each set."
      ],
      "metadata": {
        "id": "1rVM7MK_bqfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
        "FILENAME = \"aclImdb_v1.tar.gz\"\n",
        "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
        "path = Path(filepath).parent / \"aclImdb\"\n",
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5TBBM_jb0jd",
        "outputId": "d78547cc-e1f9-4886-e249-dde427dbede0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/aclImdb')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Hgyf_O-Q8ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wJkwINCnQ88B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for name, subdirs, files in os.walk(path):\n",
        "    indent = len(Path(name).parts) - len(path.parts)\n",
        "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
        "    for index, filename in enumerate(sorted(files)):\n",
        "        if index == 3:\n",
        "            print(\"    \" * (indent + 1) + \"...\")\n",
        "            break\n",
        "        print(\"    \" * (indent + 1) + filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjGzcy8-b0hG",
        "outputId": "9878e072-d4d7-4a16-ac49-02a09f65156b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aclImdb/\n",
            "    README\n",
            "    imdb.vocab\n",
            "    imdbEr.txt\n",
            "    test/\n",
            "        labeledBow.feat\n",
            "        urls_neg.txt\n",
            "        urls_pos.txt\n",
            "        pos/\n",
            "            0_10.txt\n",
            "            10000_7.txt\n",
            "            10001_9.txt\n",
            "            ...\n",
            "        neg/\n",
            "            0_2.txt\n",
            "            10000_4.txt\n",
            "            10001_1.txt\n",
            "            ...\n",
            "    train/\n",
            "        labeledBow.feat\n",
            "        unsupBow.feat\n",
            "        urls_neg.txt\n",
            "        ...\n",
            "        unsup/\n",
            "            0_0.txt\n",
            "            10000_0.txt\n",
            "            10001_0.txt\n",
            "            ...\n",
            "        pos/\n",
            "            0_9.txt\n",
            "            10000_8.txt\n",
            "            10001_10.txt\n",
            "            ...\n",
            "        neg/\n",
            "            0_3.txt\n",
            "            10000_4.txt\n",
            "            10001_4.txt\n",
            "            ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_paths(dirpath):\n",
        "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
        "\n",
        "train_pos = review_paths(path / \"train\" / \"pos\")\n",
        "train_neg = review_paths(path / \"train\" / \"neg\")\n",
        "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
        "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
        "\n",
        "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FEzFFCEb0d2",
        "outputId": "b50cca86-bfc8-4721-fc2c-282fe03c3b01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500, 12500, 12500, 12500)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(test_valid_pos)\n",
        "\n",
        "test_pos = test_valid_pos[:5000]\n",
        "test_neg = test_valid_neg[:5000]\n",
        "valid_pos = test_valid_pos[5000:]\n",
        "valid_neg = test_valid_neg[5000:]"
      ],
      "metadata": {
        "id": "wZKLnwmsb0bS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
        "  positive_reviews_ds = tf.data.TextLineDataset(filepaths_positive, num_parallel_reads=n_read_threads)\n",
        "  positive_reviews_ds = positive_reviews_ds.map(lambda review: (review, 1))\n",
        "\n",
        "  negative_reviews_ds = tf.data.TextLineDataset(filepaths_negative, num_parallel_reads=n_read_threads)\n",
        "  negative_reviews_ds = negative_reviews_ds.map(lambda review: (review, 0))\n",
        "  \n",
        "  return tf.data.Dataset.concatenate(positive_reviews_ds, negative_reviews_ds)"
      ],
      "metadata": {
        "id": "qrUF-M0uqQx0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_ds = tf.data.TextLineDataset(train_pos[0:3], num_parallel_reads=5)"
      ],
      "metadata": {
        "id": "ax5bk_6GjOjm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(example_ds.as_numpy_iterator())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQTM_ifUjUcd",
        "outputId": "94d3a21c-ff82-4d63-d6d8-88b87db7d0a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b\"CRY FREEDOM is an excellent primer for those wanting an overview of apartheid's cruelty in just a couple of hours. Famed director Richard Attenborough (GANDHI) is certainly no stranger to the genre, and the collaboration of the real-life Mr. and Mrs. Woods, the main white characters in their book and in this film, lends further authenticity to CRY FREEDOM. The video now in release actually runs a little over 2 and a half hours since 23 minutes of extra footage was inserted to make it a two part TV miniseries after the film's initial theatrical release. While the added length serves to heighten the film's forgivable flaws: uneven character development and blanket stereotyping in particular, another possible flaw (the insistence on the white characters' fate over that of the African ones) may work out as a strength. Viewing CRYING FREEDOM as a politically and historically educational film (as I think it should, over its artistic merits), the story is one which black Africans know only too well, though the younger generation may now need to see it on film for full impact. It is the whites who have always been the film's and the book's target audience, hopefully driving them to change. Now twelve years after the movie's production, CRY FREEDOM is in many ways a more interesting film to watch. Almost ten years after black majority rule has been at least theorically in place, 1987's CRY FREEDOM's ideals remain by and large unrealized. It therefore remains as imperative as ever for white South Africans, particularly the younger ones who have only heard of these actions to see it, and absorb the film's messages. In total contrast to American slavery and the Jewish Holocaust's exposure, South Africans' struggles have been told by a mere two or three stories: CRY FREEDOM, CRY THE BELOVED COUNTRY (OK, Count it twice if you include the remake), and SARAFINA (did I miss one?). All three dramas also clumsily feature American and British actors in both the white and black roles. Not one South African actor has played a major role, white, coloured, Indian or Black!). And yes I did miss another international South African drama, MANDELA and DEKLERK. Though this (also highly recommended) biopic was released after black majority rule was instituted, MANDELA was played by a Black American (Sidney Poitier, who also starred in the original S.A.-themed CRY THE BELOVED COUNTRY), while the Afrikaner DeKlerk was played by a (bald) very British Michael Caine, a good performance if you can dismiss that the very essence of Afrikanerdom is vehement anti-British feelings. Until local SABC TV and African films start dealing with their own legacy, CRY FREEDOM is about as authentic as you'll get. As villified as the whites (particularly the Afrikaners) are portrayed in the film, any observant (non-casual) visitor to South Africa even now in 1999, not to mention 1977 when CRY FREEDOM takes place, will generally find white's attitudes towards blacks restrained, even understated. Looking at CRY FREEDOM in hindsight, it is amazing that reconciliation can take place at all, and it is. But CRY FREEDOM at time shows not much has really changed in many people's minds yet, and that the Black Africans' goal to FREEDOM and reconciliation is still ongoing. This is why if you're a novice to the situation, CRY FREEDOM, is your best introduction.\",\n",
              " b'Though the story is essentially routine, and the \"surprise\" ending is nothing but a bad joke on the audience, you can see what attracted these good actors to the project - it offers them the kind of roles in which good actors can shine, and shine they do. The film is impeccably made - for its time. It was remade in 2000 as \"Under Suspicion\" and if you only want to see one version of the story (that\\'s all it deserves, really), I recommend the latter one, with Hopkins\\' up-to-date direction and the more explicit references to plot points that the original could only hint at. The ending, however, still blows. (**1/2)',\n",
              " b\"The master of movie spectacle Cecil B. De Mille goes West. Using three legends of the old west as its protagonists (they probably never met),Gary Cooper is portraying Wild Bill Hickock,James Ellison as Buffalo Bill and Jean Arthur does make a nice Calamity Jane. The story serves only for De Mille to hang some marvelous action sequences on, like the big Indian attack.Scenes like that are extremely well done.If you don't mind the somewhat over-the-top performances of the cast this is an very entertaining western.Look out for a very young Anthony Quinn essaying the role of an Indian brave who participated at the battle of Little Big Horn.This part got him at least noticed in Hollywood.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = imdb_dataset(train_pos, train_neg)\n",
        "valid_ds = imdb_dataset(valid_pos, valid_neg)\n",
        "test_ds = imdb_dataset(test_pos, test_neg)"
      ],
      "metadata": {
        "id": "AjeZTQmOibjL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfLclWL-pKrQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.shuffle(25000).batch(batch_size).prefetch(1)\n",
        "valid_ds = valid_ds.batch(batch_size).prefetch(1)\n",
        "test_ds = test_ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "Z2e5-cXgpGKF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART D.\n",
        "\n",
        "Create a binary classification model, using a TextVectorization layer to preprocess each review. If the TextVectorization layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the tf.strings package, for example lower() to make everything lowercase, regex_replace() to replace punctuation with spaces, and split() to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the adapt() method."
      ],
      "metadata": {
        "id": "v4xrMx-3mwdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  no_punctuation = tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "  return no_punctuation\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens = 1000,\n",
        "    standardize = custom_standardization\n",
        ")\n",
        "  "
      ],
      "metadata": {
        "id": "m3X0Nv_aibg3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this was kind of slow, so we'll do the sample thing\n",
        "#text_vectorizer.adapt(train_ds.map(lambda review, label: review))\n",
        "\n",
        "sample_train_ds = train_ds.take(500)\n",
        "\n",
        "text_vectorizer.adapt(sample_train_ds.map(lambda review, label: review))"
      ],
      "metadata": {
        "id": "f43CeT_qibei"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Had a heck of a time getting this to run!\n",
        "\n",
        "class Bag_of_words_layer(keras.layers.Layer):\n",
        "  def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
        "    super().__init__(trainable = False, dtype=dtype, **kwargs)\n",
        "    self.n_tokens = n_tokens\n",
        "\n",
        "  def call(self, inputs):\n",
        "    one_hot_encoded = tf.one_hot(inputs, self.n_tokens)\n",
        "    return tf.reduce_sum(one_hot_encoded, axis=1)"
      ],
      "metadata": {
        "id": "5H7ww47k-zJe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o9mGvovd-U-V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    Bag_of_words_layer(len(text_vectorizer.get_vocabulary())),\n",
        "    keras.layers.Dense(100),\n",
        "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
        "]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "mI7XATK2-U7U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs=5, validation_data=valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e_a3Gi7-U4t",
        "outputId": "9feb2e92-85dc-4495-fd0d-e6d4c2767ecf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 89s 108ms/step - loss: 0.7327 - accuracy: 0.7594 - val_loss: 0.3824 - val_accuracy: 0.8543\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 74s 82ms/step - loss: 0.5725 - accuracy: 0.8181 - val_loss: 0.4686 - val_accuracy: 0.8387\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 70s 85ms/step - loss: 0.5055 - accuracy: 0.8254 - val_loss: 0.4151 - val_accuracy: 0.8415\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 64s 79ms/step - loss: 0.4687 - accuracy: 0.8300 - val_loss: 0.3765 - val_accuracy: 0.8537\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 67s 82ms/step - loss: 0.4427 - accuracy: 0.8353 - val_loss: 0.4284 - val_accuracy: 0.8322\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f86eca24050>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsqPrbWR-U19"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PART E.\n",
        "\n",
        "Add an Embedding layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model."
      ],
      "metadata": {
        "id": "4Wuj0I5qK82Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9rO9fHm-Uy9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to compute: number of words in review, excluding zero vectors; mean across words.\n"
      ],
      "metadata": {
        "id": "U2Lo-21MPxtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't understand the errors I am getting so I will recreate the pieces from scratch."
      ],
      "metadata": {
        "id": "mC3d_a3r86Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  no_punctuation = tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "  return no_punctuation\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens = 1000,\n",
        "    standardize = custom_standardization\n",
        ")"
      ],
      "metadata": {
        "id": "K8zAa6ga86NF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train_ds = train_ds.take(500)\n",
        "\n",
        "text_vectorizer.adapt(sample_train_ds.map(lambda review, label: review))"
      ],
      "metadata": {
        "id": "pR-tLElO86Kv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 16\n",
        "\n",
        "embedding_layer = keras.layers.Embedding(input_dim = len(text_vectorizer.get_vocabulary()), \n",
        "                           output_dim = embedding_dim, mask_zero=True)"
      ],
      "metadata": {
        "id": "doC1kmMx9Zgy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    embedding_layer,\n",
        "    keras.layers.Lambda(lambda x: tf.math.reduce_mean(x, axis=1)),\n",
        "    keras.layers.Dense(100),\n",
        "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_ds, epochs=5, validation_data=valid_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5QAJGBD86D2",
        "outputId": "6f84d317-bffb-441c-8451-fa568d713687"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 24s 22ms/step - loss: 0.5457 - accuracy: 0.7150 - val_loss: 0.4435 - val_accuracy: 0.7789\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 23s 25ms/step - loss: 0.3691 - accuracy: 0.8499 - val_loss: 0.3618 - val_accuracy: 0.8509\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 20s 22ms/step - loss: 0.3441 - accuracy: 0.8596 - val_loss: 0.3406 - val_accuracy: 0.8607\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 20s 20ms/step - loss: 0.3320 - accuracy: 0.8644 - val_loss: 0.3371 - val_accuracy: 0.8621\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 17s 18ms/step - loss: 0.3274 - accuracy: 0.8681 - val_loss: 0.3359 - val_accuracy: 0.8622\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f86ec202b50>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nAiKkxTLibZu"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}